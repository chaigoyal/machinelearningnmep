
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Data Day}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Bias Variance}\label{bias-variance}

    \subsection{Import stuff}\label{import-stuff}

    We'll need: pandas, numpy, matplotlib, sklearn.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}143}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{k+kn}{import} \PY{n+nn}{matplotlib} \PY{k}{as} \PY{n+nn}{plt}
          \PY{k+kn}{import} \PY{n+nn}{sklearn}
          \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
          \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
          \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{neighbors}
\end{Verbatim}


    \subsection{Get the data}\label{get-the-data}

    Dataset URL =
"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}121}]:} \PY{n}{a} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iris.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{c} \PY{o}{=} \PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{a}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}121}]:}      sepal\_length  sepal\_width  petal\_length  petal\_width    species
          0             5.1          3.5           1.4          0.2     setosa
          1             4.9          3.0           1.4          0.2     setosa
          2             4.7          3.2           1.3          0.2     setosa
          3             4.6          3.1           1.5          0.2     setosa
          4             5.0          3.6           1.4          0.2     setosa
          5             5.4          3.9           1.7          0.4     setosa
          6             4.6          3.4           1.4          0.3     setosa
          7             5.0          3.4           1.5          0.2     setosa
          8             4.4          2.9           1.4          0.2     setosa
          9             4.9          3.1           1.5          0.1     setosa
          10            5.4          3.7           1.5          0.2     setosa
          11            4.8          3.4           1.6          0.2     setosa
          12            4.8          3.0           1.4          0.1     setosa
          13            4.3          3.0           1.1          0.1     setosa
          14            5.8          4.0           1.2          0.2     setosa
          15            5.7          4.4           1.5          0.4     setosa
          16            5.4          3.9           1.3          0.4     setosa
          17            5.1          3.5           1.4          0.3     setosa
          18            5.7          3.8           1.7          0.3     setosa
          19            5.1          3.8           1.5          0.3     setosa
          20            5.4          3.4           1.7          0.2     setosa
          21            5.1          3.7           1.5          0.4     setosa
          22            4.6          3.6           1.0          0.2     setosa
          23            5.1          3.3           1.7          0.5     setosa
          24            4.8          3.4           1.9          0.2     setosa
          25            5.0          3.0           1.6          0.2     setosa
          26            5.0          3.4           1.6          0.4     setosa
          27            5.2          3.5           1.5          0.2     setosa
          28            5.2          3.4           1.4          0.2     setosa
          29            4.7          3.2           1.6          0.2     setosa
          ..            {\ldots}          {\ldots}           {\ldots}          {\ldots}        {\ldots}
          120           6.9          3.2           5.7          2.3  virginica
          121           5.6          2.8           4.9          2.0  virginica
          122           7.7          2.8           6.7          2.0  virginica
          123           6.3          2.7           4.9          1.8  virginica
          124           6.7          3.3           5.7          2.1  virginica
          125           7.2          3.2           6.0          1.8  virginica
          126           6.2          2.8           4.8          1.8  virginica
          127           6.1          3.0           4.9          1.8  virginica
          128           6.4          2.8           5.6          2.1  virginica
          129           7.2          3.0           5.8          1.6  virginica
          130           7.4          2.8           6.1          1.9  virginica
          131           7.9          3.8           6.4          2.0  virginica
          132           6.4          2.8           5.6          2.2  virginica
          133           6.3          2.8           5.1          1.5  virginica
          134           6.1          2.6           5.6          1.4  virginica
          135           7.7          3.0           6.1          2.3  virginica
          136           6.3          3.4           5.6          2.4  virginica
          137           6.4          3.1           5.5          1.8  virginica
          138           6.0          3.0           4.8          1.8  virginica
          139           6.9          3.1           5.4          2.1  virginica
          140           6.7          3.1           5.6          2.4  virginica
          141           6.9          3.1           5.1          2.3  virginica
          142           5.8          2.7           5.1          1.9  virginica
          143           6.8          3.2           5.9          2.3  virginica
          144           6.7          3.3           5.7          2.5  virginica
          145           6.7          3.0           5.2          2.3  virginica
          146           6.3          2.5           5.0          1.9  virginica
          147           6.5          3.0           5.2          2.0  virginica
          148           6.2          3.4           5.4          2.3  virginica
          149           5.9          3.0           5.1          1.8  virginica
          
          [150 rows x 5 columns]
\end{Verbatim}
            
    \subsection{Import stuff for Train-Test
Split}\label{import-stuff-for-train-test-split}

    We'll need: train\_test\_split

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
\end{Verbatim}


    Determine X\_train, X\_test, y\_train, y\_test using sklearn.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}123}]:} \PY{c+c1}{\PYZsh{} determine X and y}
          \PY{n}{X} \PY{o}{=} \PY{n}{a}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{y} \PY{o}{=} \PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} split into train and test}
          \PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(112, 4)

    \end{Verbatim}

    \subsection{Import stuff for kNN}\label{import-stuff-for-knn}

    We'll need: KNeighborsClassifier, accuracy\_score.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}124}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
\end{Verbatim}


    \subsection{High Bias}\label{high-bias}

    Let's start by training a K Nearest Neighbors classifier with high bias.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}126}]:} \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{110}\PY{p}{)}
          \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{y\PYZus{}predicted} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
          
          \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predicted}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}126}]:} 0.5
\end{Verbatim}
            
    \paragraph{What happened here?}\label{what-happened-here}

    With a super high number of n\_numbers (110 in this case), the
classifier has a very high bias. Therefore, the overall accuracy is very
low, or 47.3\%. Too much bias results in underfitting.

    Now let's train a K Nearest Neighbors classifier with high variance.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}129}]:} \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{y\PYZus{}predicted} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
          
          \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predicted}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}129}]:} 0.9736842105263158
\end{Verbatim}
            
    \paragraph{What happened here?}\label{what-happened-here}

    Since the n\_neighbors value is super low, the variance is very high.
However, the data is still quite accurate, with an accuracy score of
92\%. Optimizing on only 1 nearest point means that the noise is modeled
very high. Shuffling the data will greatly change the model every time.

    Can we do better? Try experimenting with the value of K.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}144}]:} \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{110}\PY{p}{)}\PY{p}{:} 
              \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{k}\PY{p}{)}
              \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
              \PY{n}{y\PYZus{}predicted} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
              \PY{n}{a}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{[}\PY{n}{k}\PY{p}{,}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predicted}\PY{p}{)}\PY{p}{]}\PY{p}{)}
              \PY{n}{y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}predicted}\PY{p}{)}\PY{p}{)}
              
          
          \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{109}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{110}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}the maximum accuracy is when k=2; look at matrix \PYZdq{}x\PYZdq{}}
          
          \PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
          \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{y\PYZus{}predicted} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
          
          \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predicted}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/api.py:107: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects
  result = result.union(other)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        TypeError                                 Traceback (most recent call last)

        <ipython-input-144-2cc271a8c7ab> in <module>()
          4     y\_predicted = classifier.predict(X\_test)
          5     a.append([k,accuracy\_score(y\_test,y\_predicted)])
    ----> 6     y.append(accuracy\_score(y\_test,y\_predicted))
          7 
          8 


        /anaconda3/lib/python3.6/site-packages/pandas/core/series.py in append(self, to\_append, ignore\_index, verify\_integrity)
       2152             to\_concat = [self, to\_append]
       2153         return concat(to\_concat, ignore\_index=ignore\_index,
    -> 2154                       verify\_integrity=verify\_integrity)
       2155 
       2156     def \_binop(self, other, func, level=None, fill\_value=None):


        /anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py in concat(objs, axis, join, join\_axes, ignore\_index, keys, levels, names, verify\_integrity, sort, copy)
        223                        keys=keys, levels=levels, names=names,
        224                        verify\_integrity=verify\_integrity,
    --> 225                        copy=copy, sort=sort)
        226     return op.get\_result()
        227 


        /anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py in \_\_init\_\_(self, objs, axis, join, join\_axes, keys, levels, names, ignore\_index, verify\_integrity, copy, sort)
        284                        ' only pd.Series, pd.DataFrame, and pd.Panel'
        285                        ' (deprecated) objs are valid'.format(type(obj)))
    --> 286                 raise TypeError(msg)
        287 
        288             \# consolidate


        TypeError: cannot concatenate object of type "<class 'numpy.float64'>"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid

    \end{Verbatim}

    \paragraph{What happened here?}\label{what-happened-here}

    The maximum accuracy\_score was recieved when k=2.

    \paragraph{Explain the Bias-Variance
tradeoff.}\label{explain-the-bias-variance-tradeoff.}

    This tradeoff represents the struggle to find a value that minimizes
both sources of error - bias and variance. With a high variance, the
bias is near 0, which means that we aren't doing a good job of
predicting the data values in the test data. With a very high bias, we
aren't correctly finding trends in the data and are making a super
general model.

    \paragraph{How do bias and variance affect training and testing
error?}\label{how-do-bias-and-variance-affect-training-and-testing-error}

    High variance means that the data is overfitted; it doesn't fit well on
a cross-validation set. A high bias would imply underfitting, where the
model is not fit well to basically any data. High variance implies a low
training error, but high testing error. Bias is vice-cersa.

    \section{K-Fold Cross Validation}\label{k-fold-cross-validation}

    \paragraph{What is K-Fold Cross
Validation?}\label{what-is-k-fold-cross-validation}

    KFCV splits a group into k groups. Cross-validation is used to estimate
a model's skill on unseen data. There's always a tradeoff when you want
more test sets, which would mean less training sets.

If you have 200 data points, split it into 10 bins, so 20 points per
bin. Run k separate learning experiments. Split each 20 into test and
training sets. Average out the test results from the k experiments.

    \paragraph{How can we use K-Fold Cross Validation to determine the
optimal value of K to use in
kNN?}\label{how-can-we-use-k-fold-cross-validation-to-determine-the-optimal-value-of-k-to-use-in-knn}

    First find a value that lowers the variance. Don't choose a very large K
otherwise that would limit the number of iterations that are possible. A
larger K means less bias towards overestimating but a higher variance
(and less efficient).

    \subsection{Import stuff for K-Fold Cross
Validation}\label{import-stuff-for-k-fold-cross-validation}

    We'll need: cross\_val\_score.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}132}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{KFold}
\end{Verbatim}


    Now let's use cross validation to tune the hyperparameter K (the number
of neighbors to consider). We want to choose the value of K that
minimizes the test error.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}148}]:} \PY{c+c1}{\PYZsh{} list of possible K\PYZhy{}values for KNN}
          \PY{n}{k\PYZus{}values} \PY{o}{=} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{]}
          
          \PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    Plot the accuracy of the classifier for each value of K.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}154}]:} \PY{n}{e} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{f} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{average} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{k\PYZus{}values}\PY{p}{:}
              \PY{n}{KNC} \PY{o}{=} \PY{n}{neighbors}\PY{o}{.}\PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{n}{i}\PY{p}{)}
              \PY{n}{KNC}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
              \PY{n}{e}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
              \PY{n}{f}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{KNC}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
              \PY{n}{best} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{KNC}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{)}
              \PY{n}{average}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{best}\PY{p}{)}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{e}\PY{p}{,} \PY{n}{average}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        ValueError                                Traceback (most recent call last)

        <ipython-input-154-504f3f0fa5b5> in <module>()
          7     e.append(i)
          8     f.append(KNC.score(X\_test,y\_test))
    ----> 9     best = cross\_val\_score(KNC,X\_test,y\_test)
         10     average.append(np.mean(best))
         11 


        /anaconda3/lib/python3.6/site-packages/sklearn/model\_selection/\_validation.py in cross\_val\_score(estimator, X, y, groups, scoring, cv, n\_jobs, verbose, fit\_params, pre\_dispatch)
        340                                 n\_jobs=n\_jobs, verbose=verbose,
        341                                 fit\_params=fit\_params,
    --> 342                                 pre\_dispatch=pre\_dispatch)
        343     return cv\_results['test\_score']
        344 


        /anaconda3/lib/python3.6/site-packages/sklearn/model\_selection/\_validation.py in cross\_validate(estimator, X, y, groups, scoring, cv, n\_jobs, verbose, fit\_params, pre\_dispatch, return\_train\_score)
        204             fit\_params, return\_train\_score=return\_train\_score,
        205             return\_times=True)
    --> 206         for train, test in cv.split(X, y, groups))
        207 
        208     if return\_train\_score:


        /anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in \_\_call\_\_(self, iterable)
        777             \# was dispatched. In particular this covers the edge
        778             \# case of Parallel used with an exhausted iterator.
    --> 779             while self.dispatch\_one\_batch(iterator):
        780                 self.\_iterating = True
        781             else:


        /anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch\_one\_batch(self, iterator)
        623                 return False
        624             else:
    --> 625                 self.\_dispatch(tasks)
        626                 return True
        627 


        /anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in \_dispatch(self, batch)
        586         dispatch\_timestamp = time.time()
        587         cb = BatchCompletionCallBack(dispatch\_timestamp, len(batch), self)
    --> 588         job = self.\_backend.apply\_async(batch, callback=cb)
        589         self.\_jobs.append(job)
        590 


        /anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/\_parallel\_backends.py in apply\_async(self, func, callback)
        109     def apply\_async(self, func, callback=None):
        110         """Schedule a func to be run"""
    --> 111         result = ImmediateResult(func)
        112         if callback:
        113             callback(result)


        /anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/\_parallel\_backends.py in \_\_init\_\_(self, batch)
        330         \# Don't delay the application, to avoid keeping the input
        331         \# arguments in memory
    --> 332         self.results = batch()
        333 
        334     def get(self):


        /anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in \_\_call\_\_(self)
        129 
        130     def \_\_call\_\_(self):
    --> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        132 
        133     def \_\_len\_\_(self):


        /anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0)
        129 
        130     def \_\_call\_\_(self):
    --> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        132 
        133     def \_\_len\_\_(self):


        /anaconda3/lib/python3.6/site-packages/sklearn/model\_selection/\_validation.py in \_fit\_and\_score(estimator, X, y, scorer, train, test, verbose, parameters, fit\_params, return\_train\_score, return\_parameters, return\_n\_test\_samples, return\_times, error\_score)
        486         fit\_time = time.time() - start\_time
        487         \# \_score will return dict if is\_multimetric is True
    --> 488         test\_scores = \_score(estimator, X\_test, y\_test, scorer, is\_multimetric)
        489         score\_time = time.time() - start\_time - fit\_time
        490         if return\_train\_score:


        /anaconda3/lib/python3.6/site-packages/sklearn/model\_selection/\_validation.py in \_score(estimator, X\_test, y\_test, scorer, is\_multimetric)
        521     """
        522     if is\_multimetric:
    --> 523         return \_multimetric\_score(estimator, X\_test, y\_test, scorer)
        524     else:
        525         if y\_test is None:


        /anaconda3/lib/python3.6/site-packages/sklearn/model\_selection/\_validation.py in \_multimetric\_score(estimator, X\_test, y\_test, scorers)
        551             score = scorer(estimator, X\_test)
        552         else:
    --> 553             score = scorer(estimator, X\_test, y\_test)
        554 
        555         if hasattr(score, 'item'):


        /anaconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py in \_passthrough\_scorer(estimator, *args, **kwargs)
        242 def \_passthrough\_scorer(estimator, *args, **kwargs):
        243     """Function that wraps estimator.score"""
    --> 244     return estimator.score(*args, **kwargs)
        245 
        246 


        /anaconda3/lib/python3.6/site-packages/sklearn/base.py in score(self, X, y, sample\_weight)
        347         """
        348         from .metrics import accuracy\_score
    --> 349         return accuracy\_score(y, self.predict(X), sample\_weight=sample\_weight)
        350 
        351 


        /anaconda3/lib/python3.6/site-packages/sklearn/neighbors/classification.py in predict(self, X)
        143         X = check\_array(X, accept\_sparse='csr')
        144 
    --> 145         neigh\_dist, neigh\_ind = self.kneighbors(X)
        146 
        147         classes\_ = self.classes\_


        /anaconda3/lib/python3.6/site-packages/sklearn/neighbors/base.py in kneighbors(self, X, n\_neighbors, return\_distance)
        345                 "Expected n\_neighbors <= n\_samples, "
        346                 " but n\_samples = \%d, n\_neighbors = \%d" \%
    --> 347                 (train\_size, n\_neighbors)
        348             )
        349         n\_samples, \_ = X.shape


        ValueError: Expected n\_neighbors <= n\_samples,  but n\_samples = 25, n\_neighbors = 26

    \end{Verbatim}

    What is the optimal value of K found using 20-fold cross validation?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}156}]:} \PY{c+c1}{\PYZsh{}10 is generally an optimal value}
\end{Verbatim}


    \paragraph{Explain stratified cross
validation.}\label{explain-stratified-cross-validation.}

    This involves selecting folds so that the mean response value of the
folds is approximately equal in all the folds.

    \section{Grid Search}\label{grid-search}

    \paragraph{What is Grid Search?}\label{what-is-grid-search}

    Grid Search involves having a set of models with different parameter
values, and then evaluating them with a cross-validation to select the
most accurate one.

    \subsection{Import stuff for
GridSearch}\label{import-stuff-for-gridsearch}

    We'll need: GridSearchCV.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}157}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
          \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{svm}\PY{p}{,} \PY{n}{datasets}
\end{Verbatim}


    Try using GridSearch to determine the optimal value of K.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}161}]:} \PY{c+c1}{\PYZsh{} list of possible k values for KNN}
          \PY{n}{k\PYZus{}values} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{)}
          \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{k\PYZus{}values}\PY{p}{\PYZcb{}}
          
          \PY{n}{clf} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{KNC}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
          \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}161}]:} GridSearchCV(cv=None, error\_score='raise',
                 estimator=KNeighborsClassifier(algorithm='auto', leaf\_size=30, metric='minkowski',
                     metric\_params=None, n\_jobs=1, n\_neighbors=26, p=2,
                     weights='uniform'),
                 fit\_params=None, iid=True, n\_jobs=1,
                 param\_grid=\{'n\_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\},
                 pre\_dispatch='2*n\_jobs', refit=True, return\_train\_score='warn',
                 scoring=None, verbose=0)
\end{Verbatim}
            

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
